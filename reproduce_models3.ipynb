{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Standard_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgGheJCNKKMi",
        "outputId": "791bf596-f544-42e8-8366-5f2363fb9e13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "#sys.path.append('/content/gdrive/hipsternet-master')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCu02wlCPYnd"
      },
      "source": [
        "sys.path.insert(0,'/content/drive/My Drive/hipsternet-master')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN3bBYMlN5pZ"
      },
      "source": [
        "import hipsternet"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9kb53pqP3zh"
      },
      "source": [
        "import numpy as np\n",
        "import hipsternet.input_data as input_data\n",
        "import hipsternet.neuralnet as nn\n",
        "from hipsternet.solver import *"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXvo4y6dk_T5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import sympy\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import collections\n",
        "import torch\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from __future__ import print_function\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "# visualization tools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsz5TNAzlC8t"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "sBqsQBIoazGU",
        "outputId": "b6bcf99a-5f39-4fcd-bcd3-91c4c1d51819"
      },
      "source": [
        "import tensorflow_datasets as tfds \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Generating rotated images: rotated 15%, fill empty pixal with closest color\n",
        "datagen = ImageDataGenerator(rotation_range=15,horizontal_flip=False,fill_mode='nearest')\n",
        "datagen.fit(x_train.reshape(x_train.shape[0],28,28,1))\n",
        "\n",
        "#Print the first rotated image\n",
        "print('Before rotating 15%')\n",
        "plt.imshow(x_train[0])\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print('After rotating 15%')\n",
        "for X, Y in datagen.flow(x_train.reshape(x_train.shape[0], 28, 28, 1),y_train.reshape(y_train.shape[0], 1),batch_size=32,shuffle=False):\n",
        "        plt.imshow(X[0].reshape(28,28))\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "#append new images to training set: image size should be at least 3D, so reshape is necessary here\n",
        "i=0\n",
        "x_train_aug = x_train\n",
        "y_train_aug = y_train\n",
        "for X, Y in datagen.flow(x_train.reshape(x_train.shape[0], 28, 28, 1),y_train.reshape(y_train.shape[0], 1),batch_size=32,shuffle=False):\n",
        "        x_train_aug = np.vstack([x_train_aug, X.reshape(X.shape[0],28,28)])\n",
        "        y_train_aug = np.append(y_train_aug, Y)\n",
        "        i += 1\n",
        "        if (i > x_train.shape[0]/32): break #fix here"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before rotating 15%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "After rotating 15%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W3D61sza61Z",
        "outputId": "2f787d7f-aefa-4a28-ea09-b9e3eaf3010c"
      },
      "source": [
        "#Augmented training sets: x_train_aug, y_train_aug\n",
        "#vectorize data into 2-d np array\n",
        "x_train, x_test = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]), x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "#normalization: rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "print(\"Dimansion of x_train: \", x_train.shape)\n",
        "\n",
        "#reshape and normalize the augmented training set\n",
        "x_train_aug = x_train_aug.reshape(x_train_aug.shape[0], x_train_aug.shape[1]*x_train_aug.shape[2])\n",
        "x_train_aug = x_train_aug/255.0\n",
        "print(\"Dimansion of x_train_aug: \", x_train_aug.shape)\n",
        "\n",
        "#take the first 100 as validation set\n",
        "x_validation = x_train[:100]\n",
        "y_validation = y_train[:100]\n",
        "x_train = x_train[101:]\n",
        "y_train = y_train[101:]\n",
        "\n",
        "#unormalized data\n",
        "#x_train_unormalized = x_train * 255.0\n",
        "#x_test_unormalized = x_test * 255.0\n",
        "#x_validation_unormalized = x_validation * 255.0\n",
        "\n",
        "#At this point x_train, x_test, x_train_unormalized, x_validation_unormalized, x_test_unormalized, x_validation are 2-d np array, y_train, y_test, y_validation are 1-d np array"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimansion of x_train:  (60000, 784)\n",
            "Dimansion of x_train_aug:  (120032, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9CrRTlrged"
      },
      "source": [
        "# preprocess data, normalization by substracting training mean from training, validation, test datasets\n",
        "def prepro(X_train, X_val, X_test):\n",
        "    mean = np.mean(X_train)\n",
        "    return X_train - mean, X_val - mean, X_test - mean"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIV9nDb1CNJo"
      },
      "source": [
        "Hyper-param tuning (Not used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xexn-q0OCMWh"
      },
      "source": [
        "def tune_hyperparam(model, n_iter, alpha, mb_size, n_experiment, reg, print_after, p_dropout, loss, nonlin, solver_fun, max_norm, x_train, x_val, x_test, y_train, y_val, y_test, candidates, nlayer):\n",
        "    M, D, C = x_train.shape[0], x_train.shape[1], y_train.max() + 1\n",
        "\n",
        "    accuracies = np.array([[0,0,0,0]]) # initiate accuracy array, containing hyper-param candidates composition and corresponding accuracy\n",
        "    accs = np.zeros(n_experiment) # initiate accuracy array for a single set of hyper-param\n",
        "\n",
        "    for max_norm in candidates[0]:\n",
        "      for dropout_rate_input in candidates[1]:\n",
        "        for dropout_rate_hidden in candidates[2]:\n",
        "          # composite the droup out array based on the number of hidden layers\n",
        "          dropoutArray = np.array([dropout_rate_input])\n",
        "          for i in range(1):\n",
        "            dropoutArray = np.append(dropoutArray, [dropout_rate_hidden])\n",
        "\n",
        "          print('Experiment on hyper-parm candidates. max_norm: ', max_norm, 'input drop out rate: ', dropout_rate_input, 'hidden layer drop out rate: ', dropout_rate_hidden)\n",
        "\n",
        "          for k in range(n_experiment):\n",
        "             print('Experiment-{}'.format(k + 1))\n",
        "\n",
        "             # Reset model\n",
        "             if model == 'ff':\n",
        "                net = nn.FeedForwardNet(D, C, H=2048, lam=reg, p_dropout=dropoutArray, loss=loss, nonlin=nonlin, nlayer=nlayer)\n",
        "             elif model == 'cnn':\n",
        "                net = nn.ConvNet(10, C, H=128, lam=1e-3, p_dropout=dropoutArray, loss=loss, nonlin=nonlin)\n",
        "\n",
        "             net = solver_fun(\n",
        "                 net, x_train, y_train, val_set=(x_val, y_val), mb_size=mb_size, alpha=alpha,\n",
        "                 n_iter=n_iter, print_after=print_after, max_norm=max_norm\n",
        "             )\n",
        "\n",
        "             y_pred = net.predict(x_test)\n",
        "             accs[k] = np.mean(y_pred == y_test)\n",
        "\n",
        "          print()\n",
        "          print('Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
        "          print()\n",
        "          \n",
        "          accuracies = np.concatenate((accuracies, [[max_norm, dropout_rate_input, dropout_rate_hidden, accs.mean()]]))\n",
        "\n",
        "      \n",
        "    \n",
        "    index = np.argmax(accuracies, axis=0)[3] # index of the best hyper-parm in the accuracies\n",
        "    best_max_norm = accuracies[index][0]\n",
        "    best_dropout_rate_input = accuracies[index][1]\n",
        "    best_dropout_rate_hidden = accuracies[index][2]\n",
        "    best_acc = accuracies[index][3]\n",
        "\n",
        "         \n",
        "    return best_max_norm, best_dropout_rate_input, best_dropout_rate_hidden, best_acc\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HudecacFZKBo"
      },
      "source": [
        "# Train and predict mnist dataset\n",
        "def run_mnist(model, n_iter, alpha, mb_size, n_experiment, reg, print_after, p_dropout, loss, nonlin, solver, max_norm, x_train, x_val, x_test, y_train, y_val, y_test, candidates, nlayer):\n",
        "    M, D, C = x_train.shape[0], x_train.shape[1], y_train.max() + 1\n",
        "  \n",
        "    x_train, x_val, x_test = prepro(x_train, x_val, x_test)\n",
        "    \n",
        "    # A key/value hash table of optimization algorithm, all algorithms use minibatch technique\n",
        "    solvers = dict(\n",
        "        sgd=sgd,                # SGD\n",
        "        momentum=momentum,          # Momentum SGD\n",
        "        nesterov=nesterov,          # Nesterov Momentum\n",
        "        adagrad=adagrad,           # Adagrad\n",
        "        rmsprop=rmsprop,          # RMSprop\n",
        "        adam=adam             # Adam\n",
        "    )\n",
        "    \n",
        "    solver_fun = solvers[solver]\n",
        "    \n",
        "    print()\n",
        "    print('Experimenting on {}'.format(solver))\n",
        "    print()\n",
        "    \n",
        "    if model == 'cnn':\n",
        "        img_shape = (1, 28, 28)\n",
        "        x_train = x_train.reshape(-1, *img_shape)\n",
        "        x_val = x_val.reshape(-1, *img_shape)\n",
        "        x_test = x_test.reshape(-1, *img_shape)\n",
        "\n",
        "    #tune hyper-parameters\n",
        "    best_max_norm, best_dropout_rate_input, best_dropout_rate_hidden, best_acc = tune_hyperparam(model, 500, alpha, mb_size, n_experiment, reg, print_after, p_dropout, loss, nonlin, solver_fun, max_norm, x_train, x_val, x_test, y_train, y_val, y_test, candidates, nlayer)\n",
        "\n",
        "    #train and predict\n",
        "    accs = np.zeros(n_experiment) # initiate accuracy array for a single set of hyper-param\n",
        "    best_dropoutArray = np.array([best_dropout_rate_input])\n",
        "    for i in range(1):\n",
        "        best_dropoutArray = np.append(best_dropoutArray, [best_dropout_rate_hidden])\n",
        "\n",
        "    for k in range(n_experiment):\n",
        "        print('Experiment-{}'.format(k + 1))\n",
        "\n",
        "        # Reset model\n",
        "        if model == 'ff':\n",
        "           net = nn.FeedForwardNet(D, C, H=8192, lam=reg, p_dropout=best_dropoutArray, loss=loss, nonlin=nonlin, nlayer=nlayer)\n",
        "        elif model == 'cnn':\n",
        "           net = nn.ConvNet(10, C, H=128, lam=1e-3, p_dropout=best_dropoutArray, loss=loss, nonlin=nonlin)\n",
        "\n",
        "        net = solver_fun(\n",
        "            net, x_train, y_train, val_set=(x_val, y_val), mb_size=mb_size, alpha=alpha,\n",
        "            n_iter=n_iter, print_after=print_after, max_norm=best_max_norm\n",
        "        )\n",
        "\n",
        "        y_pred = net.predict(x_test)\n",
        "        accs[k] = np.mean(y_pred == y_test)\n",
        "\n",
        "    print()\n",
        "    print('Best max_norm: ', best_max_norm, 'best dropout rate for input: ', best_dropout_rate_input, 'best dropout rate for hidden layers: ', best_dropout_rate_hidden, 'and result accuracy is: ', accs.mean())\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQqm4nrZuZXn"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3h0iTQiX2Q-"
      },
      "source": [
        "Standard 3-layer CNN with 128 hidden units\n",
        "(Conv layer with stride=1, padding=1; Maxpool; Fully connected with relu) \n",
        "+ max-norm constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoNjtH9EX-f8",
        "outputId": "1071f44e-a291-4d59-dd0b-8452c77ed699"
      },
      "source": [
        "model = 'cnn'\n",
        "n_iter = 50000\n",
        "alpha = 1e-3 #learning rate\n",
        "mb_size = 64\n",
        "n_experiment = 1\n",
        "reg = 1e-5\n",
        "print_after = 100\n",
        "p_dropout = [1] #no dropout\n",
        "loss = 'cross_ent'\n",
        "nonlin = 'relu'\n",
        "solver = 'sgd'\n",
        "max_norm = 4\n",
        "nlayer = 3\n",
        "\n",
        "# Define default candidates\n",
        "candidates = np.array([[3, 3.5, 4],[1],[1]]) # first hyper-param as max-norm, second as drop-out rate for input layer, third as drop-out rate for hidden layers\n",
        "\n",
        "run_mnist(model, n_iter, alpha, mb_size, n_experiment, reg, print_after, p_dropout, loss, nonlin, solver, max_norm, x_train, x_validation, x_test, y_train, y_validation, y_test, candidates, nlayer)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Experimenting on sgd\n",
            "\n",
            "Experiment on hyper-parm candidates. max_norm:  3 input drop out rate:  1 hidden layer drop out rate:  1\n",
            "Experiment-1\n",
            "Iter-100 loss: 2.2326 validation: 0.580000\n",
            "Iter-200 loss: 2.1494 validation: 0.740000\n",
            "Iter-300 loss: 2.0800 validation: 0.820000\n",
            "Iter-400 loss: 1.9710 validation: 0.830000\n",
            "Iter-500 loss: 1.8961 validation: 0.820000\n",
            "\n",
            "Mean accuracy: 0.7486, std: 0.0000\n",
            "\n",
            "Experiment on hyper-parm candidates. max_norm:  3.5 input drop out rate:  1 hidden layer drop out rate:  1\n",
            "Experiment-1\n",
            "Iter-100 loss: 2.2024 validation: 0.430000\n",
            "Iter-200 loss: 2.1275 validation: 0.640000\n",
            "Iter-300 loss: 1.9686 validation: 0.720000\n",
            "Iter-400 loss: 1.7386 validation: 0.730000\n",
            "Iter-500 loss: 1.6399 validation: 0.760000\n",
            "\n",
            "Mean accuracy: 0.7382, std: 0.0000\n",
            "\n",
            "Experiment on hyper-parm candidates. max_norm:  4 input drop out rate:  1 hidden layer drop out rate:  1\n",
            "Experiment-1\n",
            "Iter-100 loss: 2.1445 validation: 0.480000\n",
            "Iter-200 loss: 1.8802 validation: 0.710000\n",
            "Iter-300 loss: 1.6929 validation: 0.770000\n",
            "Iter-400 loss: 1.5326 validation: 0.790000\n",
            "Iter-500 loss: 1.3640 validation: 0.790000\n",
            "\n",
            "Mean accuracy: 0.7997, std: 0.0000\n",
            "\n",
            "Experiment-1\n",
            "Iter-100 loss: 2.0314 validation: 0.560000\n",
            "Iter-200 loss: 1.8371 validation: 0.750000\n",
            "Iter-300 loss: 1.6487 validation: 0.790000\n",
            "Iter-400 loss: 1.4824 validation: 0.820000\n",
            "Iter-500 loss: 1.1925 validation: 0.830000\n",
            "Iter-600 loss: 1.0942 validation: 0.850000\n",
            "Iter-700 loss: 0.9743 validation: 0.860000\n",
            "Iter-800 loss: 0.9036 validation: 0.850000\n",
            "Iter-900 loss: 0.7260 validation: 0.860000\n",
            "Iter-1000 loss: 0.8076 validation: 0.880000\n",
            "Iter-1100 loss: 0.6815 validation: 0.880000\n",
            "Iter-1200 loss: 0.7868 validation: 0.880000\n",
            "Iter-1300 loss: 0.7816 validation: 0.880000\n",
            "Iter-1400 loss: 0.6988 validation: 0.900000\n",
            "Iter-1500 loss: 0.6138 validation: 0.900000\n",
            "Iter-1600 loss: 0.6310 validation: 0.900000\n",
            "Iter-1700 loss: 0.5394 validation: 0.910000\n",
            "Iter-1800 loss: 0.5837 validation: 0.900000\n",
            "Iter-1900 loss: 0.5557 validation: 0.900000\n",
            "Iter-2000 loss: 0.5493 validation: 0.910000\n",
            "Iter-2100 loss: 0.4859 validation: 0.910000\n",
            "Iter-2200 loss: 0.6263 validation: 0.920000\n",
            "Iter-2300 loss: 0.5304 validation: 0.920000\n",
            "Iter-2400 loss: 0.4038 validation: 0.910000\n",
            "Iter-2500 loss: 0.5356 validation: 0.920000\n",
            "Iter-2600 loss: 0.4393 validation: 0.920000\n",
            "Iter-2700 loss: 0.4262 validation: 0.930000\n",
            "Iter-2800 loss: 0.6151 validation: 0.930000\n",
            "Iter-2900 loss: 0.4452 validation: 0.930000\n",
            "Iter-3000 loss: 0.5399 validation: 0.930000\n",
            "Iter-3100 loss: 0.5623 validation: 0.930000\n",
            "Iter-3200 loss: 0.3388 validation: 0.930000\n",
            "Iter-3300 loss: 0.4367 validation: 0.930000\n",
            "Iter-3400 loss: 0.3489 validation: 0.930000\n",
            "Iter-3500 loss: 0.4082 validation: 0.930000\n",
            "Iter-3600 loss: 0.4402 validation: 0.930000\n",
            "Iter-3700 loss: 0.4369 validation: 0.930000\n",
            "Iter-3800 loss: 0.3340 validation: 0.930000\n",
            "Iter-3900 loss: 0.3580 validation: 0.930000\n",
            "Iter-4000 loss: 0.3072 validation: 0.930000\n",
            "Iter-4100 loss: 0.4027 validation: 0.940000\n",
            "Iter-4200 loss: 0.3365 validation: 0.930000\n",
            "Iter-4300 loss: 0.3998 validation: 0.940000\n",
            "Iter-4400 loss: 0.5118 validation: 0.940000\n",
            "Iter-4500 loss: 0.4127 validation: 0.940000\n",
            "Iter-4600 loss: 0.3080 validation: 0.940000\n",
            "Iter-4700 loss: 0.4217 validation: 0.940000\n",
            "Iter-4800 loss: 0.4365 validation: 0.940000\n",
            "Iter-4900 loss: 0.3674 validation: 0.940000\n",
            "Iter-5000 loss: 0.6295 validation: 0.940000\n",
            "Iter-5100 loss: 0.3963 validation: 0.940000\n",
            "Iter-5200 loss: 0.4368 validation: 0.940000\n",
            "Iter-5300 loss: 0.3605 validation: 0.940000\n",
            "Iter-5400 loss: 0.4683 validation: 0.940000\n",
            "Iter-5500 loss: 0.3276 validation: 0.940000\n",
            "Iter-5600 loss: 0.5345 validation: 0.940000\n",
            "Iter-5700 loss: 0.3492 validation: 0.940000\n",
            "Iter-5800 loss: 0.4714 validation: 0.940000\n",
            "Iter-5900 loss: 0.4019 validation: 0.940000\n",
            "Iter-6000 loss: 0.3068 validation: 0.940000\n",
            "Iter-6100 loss: 0.4617 validation: 0.940000\n",
            "Iter-6200 loss: 0.4150 validation: 0.940000\n",
            "Iter-6300 loss: 0.3312 validation: 0.940000\n",
            "Iter-6400 loss: 0.3920 validation: 0.940000\n",
            "Iter-6500 loss: 0.4542 validation: 0.940000\n",
            "Iter-6600 loss: 0.2711 validation: 0.940000\n",
            "Iter-6700 loss: 0.6125 validation: 0.940000\n",
            "Iter-6800 loss: 0.3078 validation: 0.940000\n",
            "Iter-6900 loss: 0.4250 validation: 0.940000\n",
            "Iter-7000 loss: 0.5797 validation: 0.940000\n",
            "Iter-7100 loss: 0.4419 validation: 0.940000\n",
            "Iter-7200 loss: 0.3826 validation: 0.940000\n",
            "Iter-7300 loss: 0.2040 validation: 0.940000\n",
            "Iter-7400 loss: 0.3340 validation: 0.940000\n",
            "Iter-7500 loss: 0.2857 validation: 0.940000\n",
            "Iter-7600 loss: 0.2831 validation: 0.940000\n",
            "Iter-7700 loss: 0.4448 validation: 0.940000\n",
            "Iter-7800 loss: 0.3179 validation: 0.940000\n",
            "Iter-7900 loss: 0.4405 validation: 0.940000\n",
            "Iter-8000 loss: 0.3037 validation: 0.940000\n",
            "Iter-8100 loss: 0.3020 validation: 0.940000\n",
            "Iter-8200 loss: 0.4357 validation: 0.940000\n",
            "Iter-8300 loss: 0.4973 validation: 0.940000\n",
            "Iter-8400 loss: 0.2917 validation: 0.940000\n",
            "Iter-8500 loss: 0.2924 validation: 0.940000\n",
            "Iter-8600 loss: 0.3116 validation: 0.940000\n",
            "Iter-8700 loss: 0.1776 validation: 0.940000\n",
            "Iter-8800 loss: 0.2566 validation: 0.940000\n",
            "Iter-8900 loss: 0.2475 validation: 0.940000\n",
            "Iter-9000 loss: 0.3224 validation: 0.940000\n",
            "Iter-9100 loss: 0.4579 validation: 0.940000\n",
            "Iter-9200 loss: 0.3052 validation: 0.940000\n",
            "Iter-9300 loss: 0.4199 validation: 0.940000\n",
            "Iter-9400 loss: 0.2104 validation: 0.940000\n",
            "Iter-9500 loss: 0.2618 validation: 0.940000\n",
            "Iter-9600 loss: 0.2410 validation: 0.940000\n",
            "Iter-9700 loss: 0.2930 validation: 0.940000\n",
            "Iter-9800 loss: 0.1657 validation: 0.940000\n",
            "Iter-9900 loss: 0.2966 validation: 0.940000\n",
            "Iter-10000 loss: 0.3335 validation: 0.940000\n",
            "Iter-10100 loss: 0.2602 validation: 0.940000\n",
            "Iter-10200 loss: 0.2105 validation: 0.940000\n",
            "Iter-10300 loss: 0.5823 validation: 0.940000\n",
            "Iter-10400 loss: 0.2935 validation: 0.940000\n",
            "Iter-10500 loss: 0.4220 validation: 0.940000\n",
            "Iter-10600 loss: 0.2985 validation: 0.940000\n",
            "Iter-10700 loss: 0.2243 validation: 0.940000\n",
            "Iter-10800 loss: 0.3737 validation: 0.940000\n",
            "Iter-10900 loss: 0.3864 validation: 0.940000\n",
            "Iter-11000 loss: 0.2821 validation: 0.940000\n",
            "Iter-11100 loss: 0.3639 validation: 0.940000\n",
            "Iter-11200 loss: 0.2893 validation: 0.940000\n",
            "Iter-11300 loss: 0.4208 validation: 0.940000\n",
            "Iter-11400 loss: 0.3369 validation: 0.940000\n",
            "Iter-11500 loss: 0.2129 validation: 0.940000\n",
            "Iter-11600 loss: 0.3417 validation: 0.940000\n",
            "Iter-11700 loss: 0.3188 validation: 0.940000\n",
            "Iter-11800 loss: 0.3411 validation: 0.940000\n",
            "Iter-11900 loss: 0.2374 validation: 0.940000\n",
            "Iter-12000 loss: 0.3206 validation: 0.940000\n",
            "Iter-12100 loss: 0.3069 validation: 0.940000\n",
            "Iter-12200 loss: 0.2708 validation: 0.940000\n",
            "Iter-12300 loss: 0.1856 validation: 0.940000\n",
            "Iter-12400 loss: 0.3693 validation: 0.940000\n",
            "Iter-12500 loss: 0.3241 validation: 0.940000\n",
            "Iter-12600 loss: 0.2621 validation: 0.940000\n",
            "Iter-12700 loss: 0.1732 validation: 0.940000\n",
            "Iter-12800 loss: 0.2411 validation: 0.940000\n",
            "Iter-12900 loss: 0.2356 validation: 0.940000\n",
            "Iter-13000 loss: 0.4847 validation: 0.940000\n",
            "Iter-13100 loss: 0.2976 validation: 0.940000\n",
            "Iter-13200 loss: 0.2951 validation: 0.940000\n",
            "Iter-13300 loss: 0.3906 validation: 0.940000\n",
            "Iter-13400 loss: 0.3572 validation: 0.940000\n",
            "Iter-13500 loss: 0.1733 validation: 0.940000\n",
            "Iter-13600 loss: 0.2142 validation: 0.940000\n",
            "Iter-13700 loss: 0.2845 validation: 0.940000\n",
            "Iter-13800 loss: 0.2359 validation: 0.940000\n",
            "Iter-13900 loss: 0.2348 validation: 0.940000\n",
            "Iter-14000 loss: 0.5108 validation: 0.940000\n",
            "Iter-14100 loss: 0.3771 validation: 0.940000\n",
            "Iter-14200 loss: 0.2881 validation: 0.940000\n",
            "Iter-14300 loss: 0.3023 validation: 0.940000\n",
            "Iter-14400 loss: 0.4113 validation: 0.940000\n",
            "Iter-14500 loss: 0.3536 validation: 0.940000\n",
            "Iter-14600 loss: 0.3666 validation: 0.940000\n",
            "Iter-14700 loss: 0.3267 validation: 0.940000\n",
            "Iter-14800 loss: 0.2380 validation: 0.940000\n",
            "Iter-14900 loss: 0.2001 validation: 0.940000\n",
            "Iter-15000 loss: 0.3239 validation: 0.940000\n",
            "Iter-15100 loss: 0.2564 validation: 0.940000\n",
            "Iter-15200 loss: 0.3050 validation: 0.940000\n",
            "Iter-15300 loss: 0.2895 validation: 0.940000\n",
            "Iter-15400 loss: 0.1939 validation: 0.940000\n",
            "Iter-15500 loss: 0.3295 validation: 0.940000\n",
            "Iter-15600 loss: 0.2657 validation: 0.940000\n",
            "Iter-15700 loss: 0.3042 validation: 0.940000\n",
            "Iter-15800 loss: 0.2251 validation: 0.940000\n",
            "Iter-15900 loss: 0.2074 validation: 0.950000\n",
            "Iter-16000 loss: 0.4403 validation: 0.940000\n",
            "Iter-16100 loss: 0.2312 validation: 0.940000\n",
            "Iter-16200 loss: 0.2973 validation: 0.940000\n",
            "Iter-16300 loss: 0.2116 validation: 0.940000\n",
            "Iter-16400 loss: 0.3675 validation: 0.950000\n",
            "Iter-16500 loss: 0.1148 validation: 0.950000\n",
            "Iter-16600 loss: 0.3487 validation: 0.940000\n",
            "Iter-16700 loss: 0.3009 validation: 0.940000\n",
            "Iter-16800 loss: 0.2796 validation: 0.950000\n",
            "Iter-16900 loss: 0.2327 validation: 0.940000\n",
            "Iter-17000 loss: 0.3683 validation: 0.940000\n",
            "Iter-17100 loss: 0.2683 validation: 0.940000\n",
            "Iter-17200 loss: 0.3692 validation: 0.950000\n",
            "Iter-17300 loss: 0.2852 validation: 0.940000\n",
            "Iter-17400 loss: 0.1787 validation: 0.940000\n",
            "Iter-17500 loss: 0.2533 validation: 0.950000\n",
            "Iter-17600 loss: 0.1993 validation: 0.950000\n",
            "Iter-17700 loss: 0.2737 validation: 0.940000\n",
            "Iter-17800 loss: 0.2392 validation: 0.940000\n",
            "Iter-17900 loss: 0.2251 validation: 0.950000\n",
            "Iter-18000 loss: 0.1500 validation: 0.940000\n",
            "Iter-18100 loss: 0.2457 validation: 0.940000\n",
            "Iter-18200 loss: 0.2149 validation: 0.950000\n",
            "Iter-18300 loss: 0.2095 validation: 0.950000\n",
            "Iter-18400 loss: 0.3127 validation: 0.950000\n",
            "Iter-18500 loss: 0.3685 validation: 0.940000\n",
            "Iter-18600 loss: 0.3081 validation: 0.940000\n",
            "Iter-18700 loss: 0.1384 validation: 0.940000\n",
            "Iter-18800 loss: 0.3821 validation: 0.940000\n",
            "Iter-18900 loss: 0.3354 validation: 0.950000\n",
            "Iter-19000 loss: 0.2392 validation: 0.950000\n",
            "Iter-19100 loss: 0.2037 validation: 0.950000\n",
            "Iter-19200 loss: 0.3100 validation: 0.950000\n",
            "Iter-19300 loss: 0.2360 validation: 0.950000\n",
            "Iter-19400 loss: 0.3139 validation: 0.950000\n",
            "Iter-19500 loss: 0.2419 validation: 0.940000\n",
            "Iter-19600 loss: 0.3489 validation: 0.940000\n",
            "Iter-19700 loss: 0.1703 validation: 0.940000\n",
            "Iter-19800 loss: 0.3862 validation: 0.950000\n",
            "Iter-19900 loss: 0.1903 validation: 0.950000\n",
            "Iter-20000 loss: 0.3390 validation: 0.950000\n",
            "Iter-20100 loss: 0.2033 validation: 0.940000\n",
            "Iter-20200 loss: 0.2544 validation: 0.950000\n",
            "Iter-20300 loss: 0.3769 validation: 0.940000\n",
            "Iter-20400 loss: 0.3492 validation: 0.940000\n",
            "Iter-20500 loss: 0.3116 validation: 0.940000\n",
            "Iter-20600 loss: 0.3507 validation: 0.940000\n",
            "Iter-20700 loss: 0.2613 validation: 0.950000\n",
            "Iter-20800 loss: 0.3255 validation: 0.940000\n",
            "Iter-20900 loss: 0.2105 validation: 0.950000\n",
            "Iter-21000 loss: 0.2188 validation: 0.950000\n",
            "Iter-21100 loss: 0.3429 validation: 0.940000\n",
            "Iter-21200 loss: 0.2226 validation: 0.950000\n",
            "Iter-21300 loss: 0.2467 validation: 0.940000\n",
            "Iter-21400 loss: 0.0940 validation: 0.950000\n",
            "Iter-21500 loss: 0.1939 validation: 0.940000\n",
            "Iter-21600 loss: 0.1689 validation: 0.950000\n",
            "Iter-21700 loss: 0.3817 validation: 0.960000\n",
            "Iter-21800 loss: 0.3914 validation: 0.960000\n",
            "Iter-21900 loss: 0.1514 validation: 0.960000\n",
            "Iter-22000 loss: 0.2184 validation: 0.960000\n",
            "Iter-22100 loss: 0.1074 validation: 0.950000\n",
            "Iter-22200 loss: 0.3907 validation: 0.950000\n",
            "Iter-22300 loss: 0.2553 validation: 0.950000\n",
            "Iter-22400 loss: 0.3168 validation: 0.950000\n",
            "Iter-22500 loss: 0.2406 validation: 0.950000\n",
            "Iter-22600 loss: 0.2866 validation: 0.950000\n",
            "Iter-22700 loss: 0.2046 validation: 0.950000\n",
            "Iter-22800 loss: 0.3383 validation: 0.950000\n",
            "Iter-22900 loss: 0.4571 validation: 0.950000\n",
            "Iter-23000 loss: 0.2940 validation: 0.960000\n",
            "Iter-23100 loss: 0.2194 validation: 0.960000\n",
            "Iter-23200 loss: 0.2226 validation: 0.950000\n",
            "Iter-23300 loss: 0.2815 validation: 0.970000\n",
            "Iter-23400 loss: 0.1608 validation: 0.960000\n",
            "Iter-23500 loss: 0.3111 validation: 0.960000\n",
            "Iter-23600 loss: 0.1726 validation: 0.960000\n",
            "Iter-23700 loss: 0.1604 validation: 0.960000\n",
            "Iter-23800 loss: 0.2013 validation: 0.960000\n",
            "Iter-23900 loss: 0.1929 validation: 0.960000\n",
            "Iter-24000 loss: 0.2275 validation: 0.970000\n",
            "Iter-24100 loss: 0.2626 validation: 0.970000\n",
            "Iter-24200 loss: 0.1851 validation: 0.970000\n",
            "Iter-24300 loss: 0.3315 validation: 0.960000\n",
            "Iter-24400 loss: 0.2370 validation: 0.960000\n",
            "Iter-24500 loss: 0.3548 validation: 0.950000\n",
            "Iter-24600 loss: 0.1963 validation: 0.960000\n",
            "Iter-24700 loss: 0.2886 validation: 0.960000\n",
            "Iter-24800 loss: 0.2091 validation: 0.960000\n",
            "Iter-24900 loss: 0.2355 validation: 0.960000\n",
            "Iter-25000 loss: 0.1996 validation: 0.960000\n",
            "Iter-25100 loss: 0.1496 validation: 0.960000\n",
            "Iter-25200 loss: 0.2251 validation: 0.960000\n",
            "Iter-25300 loss: 0.1308 validation: 0.960000\n",
            "Iter-25400 loss: 0.2131 validation: 0.960000\n",
            "Iter-25500 loss: 0.2344 validation: 0.950000\n",
            "Iter-25600 loss: 0.1156 validation: 0.960000\n",
            "Iter-25700 loss: 0.1998 validation: 0.970000\n",
            "Iter-25800 loss: 0.1784 validation: 0.960000\n",
            "Iter-25900 loss: 0.3054 validation: 0.950000\n",
            "Iter-26000 loss: 0.2384 validation: 0.950000\n",
            "Iter-26100 loss: 0.3583 validation: 0.950000\n",
            "Iter-26200 loss: 0.1565 validation: 0.950000\n",
            "Iter-26300 loss: 0.2397 validation: 0.950000\n",
            "Iter-26400 loss: 0.1523 validation: 0.960000\n",
            "Iter-26500 loss: 0.3388 validation: 0.960000\n",
            "Iter-26600 loss: 0.1813 validation: 0.950000\n",
            "Iter-26700 loss: 0.2525 validation: 0.950000\n",
            "Iter-26800 loss: 0.1648 validation: 0.950000\n",
            "Iter-26900 loss: 0.2445 validation: 0.960000\n",
            "Iter-27000 loss: 0.1096 validation: 0.960000\n",
            "Iter-27100 loss: 0.3436 validation: 0.960000\n",
            "Iter-27200 loss: 0.4321 validation: 0.960000\n",
            "Iter-27300 loss: 0.0898 validation: 0.960000\n",
            "Iter-27400 loss: 0.2874 validation: 0.960000\n",
            "Iter-27500 loss: 0.2344 validation: 0.960000\n",
            "Iter-27600 loss: 0.2504 validation: 0.960000\n",
            "Iter-27700 loss: 0.2188 validation: 0.960000\n",
            "Iter-27800 loss: 0.1535 validation: 0.960000\n",
            "Iter-27900 loss: 0.2656 validation: 0.960000\n",
            "Iter-28000 loss: 0.1078 validation: 0.960000\n",
            "Iter-28100 loss: 0.1244 validation: 0.970000\n",
            "Iter-28200 loss: 0.2703 validation: 0.960000\n",
            "Iter-28300 loss: 0.2356 validation: 0.970000\n",
            "Iter-28400 loss: 0.1983 validation: 0.970000\n",
            "Iter-28500 loss: 0.2679 validation: 0.960000\n",
            "Iter-28600 loss: 0.1817 validation: 0.970000\n",
            "Iter-28700 loss: 0.2570 validation: 0.950000\n",
            "Iter-28800 loss: 0.2042 validation: 0.960000\n",
            "Iter-28900 loss: 0.3203 validation: 0.960000\n",
            "Iter-29000 loss: 0.3367 validation: 0.960000\n",
            "Iter-29100 loss: 0.2172 validation: 0.970000\n",
            "Iter-29200 loss: 0.2767 validation: 0.970000\n",
            "Iter-29300 loss: 0.1893 validation: 0.970000\n",
            "Iter-29400 loss: 0.1503 validation: 0.970000\n",
            "Iter-29500 loss: 0.2633 validation: 0.970000\n",
            "Iter-29600 loss: 0.2922 validation: 0.960000\n",
            "Iter-29700 loss: 0.1978 validation: 0.960000\n",
            "Iter-29800 loss: 0.1292 validation: 0.970000\n",
            "Iter-29900 loss: 0.2602 validation: 0.970000\n",
            "Iter-30000 loss: 0.2282 validation: 0.960000\n",
            "Iter-30100 loss: 0.2807 validation: 0.970000\n",
            "Iter-30200 loss: 0.1970 validation: 0.970000\n",
            "Iter-30300 loss: 0.3015 validation: 0.960000\n",
            "Iter-30400 loss: 0.2807 validation: 0.970000\n",
            "Iter-30500 loss: 0.1460 validation: 0.970000\n",
            "Iter-30600 loss: 0.1767 validation: 0.960000\n",
            "Iter-30700 loss: 0.1388 validation: 0.960000\n",
            "Iter-30800 loss: 0.2959 validation: 0.960000\n",
            "Iter-30900 loss: 0.1904 validation: 0.960000\n",
            "Iter-31000 loss: 0.2283 validation: 0.970000\n",
            "Iter-31100 loss: 0.2083 validation: 0.970000\n",
            "Iter-31200 loss: 0.1391 validation: 0.970000\n",
            "Iter-31300 loss: 0.1212 validation: 0.970000\n",
            "Iter-31400 loss: 0.1626 validation: 0.970000\n",
            "Iter-31500 loss: 0.1693 validation: 0.970000\n",
            "Iter-31600 loss: 0.1849 validation: 0.960000\n",
            "Iter-31700 loss: 0.3638 validation: 0.970000\n",
            "Iter-31800 loss: 0.2597 validation: 0.970000\n",
            "Iter-31900 loss: 0.2429 validation: 0.970000\n",
            "Iter-32000 loss: 0.2096 validation: 0.970000\n",
            "Iter-32100 loss: 0.2365 validation: 0.960000\n",
            "Iter-32200 loss: 0.3064 validation: 0.960000\n",
            "Iter-32300 loss: 0.1696 validation: 0.960000\n",
            "Iter-32400 loss: 0.1521 validation: 0.970000\n",
            "Iter-32500 loss: 0.2124 validation: 0.970000\n",
            "Iter-32600 loss: 0.1823 validation: 0.970000\n",
            "Iter-32700 loss: 0.2386 validation: 0.960000\n",
            "Iter-32800 loss: 0.1568 validation: 0.970000\n",
            "Iter-32900 loss: 0.3354 validation: 0.970000\n",
            "Iter-33000 loss: 0.3363 validation: 0.960000\n",
            "Iter-33100 loss: 0.2780 validation: 0.960000\n",
            "Iter-33200 loss: 0.1910 validation: 0.970000\n",
            "Iter-33300 loss: 0.2538 validation: 0.970000\n",
            "Iter-33400 loss: 0.1398 validation: 0.970000\n",
            "Iter-33500 loss: 0.2091 validation: 0.970000\n",
            "Iter-33600 loss: 0.1874 validation: 0.960000\n",
            "Iter-33700 loss: 0.1842 validation: 0.970000\n",
            "Iter-33800 loss: 0.2614 validation: 0.970000\n",
            "Iter-33900 loss: 0.1770 validation: 0.970000\n",
            "Iter-34000 loss: 0.1963 validation: 0.970000\n",
            "Iter-34100 loss: 0.2785 validation: 0.960000\n",
            "Iter-34200 loss: 0.2901 validation: 0.960000\n",
            "Iter-34300 loss: 0.1945 validation: 0.970000\n",
            "Iter-34400 loss: 0.1775 validation: 0.960000\n",
            "Iter-34500 loss: 0.1793 validation: 0.960000\n",
            "Iter-34600 loss: 0.2054 validation: 0.960000\n",
            "Iter-34700 loss: 0.4468 validation: 0.970000\n",
            "Iter-34800 loss: 0.2051 validation: 0.970000\n",
            "Iter-34900 loss: 0.1374 validation: 0.970000\n",
            "Iter-35000 loss: 0.1651 validation: 0.970000\n",
            "Iter-35100 loss: 0.1666 validation: 0.970000\n",
            "Iter-35200 loss: 0.2248 validation: 0.970000\n",
            "Iter-35300 loss: 0.2008 validation: 0.970000\n",
            "Iter-35400 loss: 0.1660 validation: 0.970000\n",
            "Iter-35500 loss: 0.1402 validation: 0.970000\n",
            "Iter-35600 loss: 0.2845 validation: 0.970000\n",
            "Iter-35700 loss: 0.1987 validation: 0.970000\n",
            "Iter-35800 loss: 0.2562 validation: 0.970000\n",
            "Iter-35900 loss: 0.1357 validation: 0.970000\n",
            "Iter-36000 loss: 0.2748 validation: 0.960000\n",
            "Iter-36100 loss: 0.2616 validation: 0.970000\n",
            "Iter-36200 loss: 0.3015 validation: 0.970000\n",
            "Iter-36300 loss: 0.1715 validation: 0.970000\n",
            "Iter-36400 loss: 0.1565 validation: 0.970000\n",
            "Iter-36500 loss: 0.2489 validation: 0.970000\n",
            "Iter-36600 loss: 0.2632 validation: 0.970000\n",
            "Iter-36700 loss: 0.1641 validation: 0.970000\n",
            "Iter-36800 loss: 0.1476 validation: 0.970000\n",
            "Iter-36900 loss: 0.1345 validation: 0.970000\n",
            "Iter-37000 loss: 0.1438 validation: 0.970000\n",
            "Iter-37100 loss: 0.2643 validation: 0.970000\n",
            "Iter-37200 loss: 0.2514 validation: 0.970000\n",
            "Iter-37300 loss: 0.1223 validation: 0.960000\n",
            "Iter-37400 loss: 0.1620 validation: 0.970000\n",
            "Iter-37500 loss: 0.1681 validation: 0.970000\n",
            "Iter-37600 loss: 0.1026 validation: 0.960000\n",
            "Iter-37700 loss: 0.1393 validation: 0.960000\n",
            "Iter-37800 loss: 0.2986 validation: 0.970000\n",
            "Iter-37900 loss: 0.2464 validation: 0.970000\n",
            "Iter-38000 loss: 0.0981 validation: 0.970000\n",
            "Iter-38100 loss: 0.1314 validation: 0.970000\n",
            "Iter-38200 loss: 0.2203 validation: 0.970000\n",
            "Iter-38300 loss: 0.2683 validation: 0.970000\n",
            "Iter-38400 loss: 0.1754 validation: 0.970000\n",
            "Iter-38500 loss: 0.1419 validation: 0.970000\n",
            "Iter-38600 loss: 0.2289 validation: 0.970000\n",
            "Iter-38700 loss: 0.1596 validation: 0.970000\n",
            "Iter-38800 loss: 0.2801 validation: 0.970000\n",
            "Iter-38900 loss: 0.2422 validation: 0.970000\n",
            "Iter-39000 loss: 0.2397 validation: 0.970000\n",
            "Iter-39100 loss: 0.2306 validation: 0.970000\n",
            "Iter-39200 loss: 0.1786 validation: 0.970000\n",
            "Iter-39300 loss: 0.2223 validation: 0.970000\n",
            "Iter-39400 loss: 0.2641 validation: 0.970000\n",
            "Iter-39500 loss: 0.2503 validation: 0.970000\n",
            "Iter-39600 loss: 0.2166 validation: 0.970000\n",
            "Iter-39700 loss: 0.1813 validation: 0.960000\n",
            "Iter-39800 loss: 0.1260 validation: 0.960000\n",
            "Iter-39900 loss: 0.1022 validation: 0.970000\n",
            "Iter-40000 loss: 0.3846 validation: 0.970000\n",
            "Iter-40100 loss: 0.2456 validation: 0.970000\n",
            "Iter-40200 loss: 0.2134 validation: 0.970000\n",
            "Iter-40300 loss: 0.2059 validation: 0.970000\n",
            "Iter-40400 loss: 0.1688 validation: 0.970000\n",
            "Iter-40500 loss: 0.1165 validation: 0.970000\n",
            "Iter-40600 loss: 0.2701 validation: 0.970000\n",
            "Iter-40700 loss: 0.2492 validation: 0.970000\n",
            "Iter-40800 loss: 0.1719 validation: 0.970000\n",
            "Iter-40900 loss: 0.1755 validation: 0.970000\n",
            "Iter-41000 loss: 0.1267 validation: 0.970000\n",
            "Iter-41100 loss: 0.2247 validation: 0.970000\n",
            "Iter-41200 loss: 0.0959 validation: 0.970000\n",
            "Iter-41300 loss: 0.3793 validation: 0.970000\n",
            "Iter-41400 loss: 0.2705 validation: 0.970000\n",
            "Iter-41500 loss: 0.1477 validation: 0.970000\n",
            "Iter-41600 loss: 0.2514 validation: 0.970000\n",
            "Iter-41700 loss: 0.2594 validation: 0.970000\n",
            "Iter-41800 loss: 0.2422 validation: 0.970000\n",
            "Iter-41900 loss: 0.1719 validation: 0.970000\n",
            "Iter-42000 loss: 0.2279 validation: 0.970000\n",
            "Iter-42100 loss: 0.1571 validation: 0.970000\n",
            "Iter-42200 loss: 0.2761 validation: 0.970000\n",
            "Iter-42300 loss: 0.1650 validation: 0.970000\n",
            "Iter-42400 loss: 0.2549 validation: 0.970000\n",
            "Iter-42500 loss: 0.3302 validation: 0.970000\n",
            "Iter-42600 loss: 0.2291 validation: 0.970000\n",
            "Iter-42700 loss: 0.1492 validation: 0.970000\n",
            "Iter-42800 loss: 0.2621 validation: 0.970000\n",
            "Iter-42900 loss: 0.1711 validation: 0.970000\n",
            "Iter-43000 loss: 0.2219 validation: 0.970000\n",
            "Iter-43100 loss: 0.1697 validation: 0.970000\n",
            "Iter-43200 loss: 0.3114 validation: 0.970000\n",
            "Iter-43300 loss: 0.1125 validation: 0.970000\n",
            "Iter-43400 loss: 0.1880 validation: 0.970000\n",
            "Iter-43500 loss: 0.1390 validation: 0.970000\n",
            "Iter-43600 loss: 0.1470 validation: 0.970000\n",
            "Iter-43700 loss: 0.2227 validation: 0.970000\n",
            "Iter-43800 loss: 0.1632 validation: 0.960000\n",
            "Iter-43900 loss: 0.2221 validation: 0.960000\n",
            "Iter-44000 loss: 0.2506 validation: 0.970000\n",
            "Iter-44100 loss: 0.2400 validation: 0.970000\n",
            "Iter-44200 loss: 0.2309 validation: 0.960000\n",
            "Iter-44300 loss: 0.2466 validation: 0.970000\n",
            "Iter-44400 loss: 0.2557 validation: 0.970000\n",
            "Iter-44500 loss: 0.1435 validation: 0.970000\n",
            "Iter-44600 loss: 0.1513 validation: 0.970000\n",
            "Iter-44700 loss: 0.1691 validation: 0.970000\n",
            "Iter-44800 loss: 0.3003 validation: 0.970000\n",
            "Iter-44900 loss: 0.2499 validation: 0.970000\n",
            "Iter-45000 loss: 0.1665 validation: 0.970000\n",
            "Iter-45100 loss: 0.1396 validation: 0.970000\n",
            "Iter-45200 loss: 0.2367 validation: 0.970000\n",
            "Iter-45300 loss: 0.2443 validation: 0.970000\n",
            "Iter-45400 loss: 0.2786 validation: 0.970000\n",
            "Iter-45500 loss: 0.1202 validation: 0.970000\n",
            "Iter-45600 loss: 0.2134 validation: 0.970000\n",
            "Iter-45700 loss: 0.2039 validation: 0.970000\n",
            "Iter-45800 loss: 0.1772 validation: 0.970000\n",
            "Iter-45900 loss: 0.2495 validation: 0.970000\n",
            "Iter-46000 loss: 0.1533 validation: 0.970000\n",
            "Iter-46100 loss: 0.2368 validation: 0.970000\n",
            "Iter-46200 loss: 0.1930 validation: 0.970000\n",
            "Iter-46300 loss: 0.2967 validation: 0.970000\n",
            "Iter-46400 loss: 0.1414 validation: 0.970000\n",
            "Iter-46500 loss: 0.1524 validation: 0.970000\n",
            "Iter-46600 loss: 0.2137 validation: 0.970000\n",
            "Iter-46700 loss: 0.1293 validation: 0.970000\n",
            "Iter-46800 loss: 0.2571 validation: 0.970000\n",
            "Iter-46900 loss: 0.1750 validation: 0.970000\n",
            "Iter-47000 loss: 0.3187 validation: 0.970000\n",
            "Iter-47100 loss: 0.1437 validation: 0.970000\n",
            "Iter-47200 loss: 0.1430 validation: 0.970000\n",
            "Iter-47300 loss: 0.1166 validation: 0.970000\n",
            "Iter-47400 loss: 0.1428 validation: 0.970000\n",
            "Iter-47500 loss: 0.2592 validation: 0.970000\n",
            "Iter-47600 loss: 0.1453 validation: 0.970000\n",
            "Iter-47700 loss: 0.1130 validation: 0.970000\n",
            "Iter-47800 loss: 0.1179 validation: 0.970000\n",
            "Iter-47900 loss: 0.2052 validation: 0.970000\n",
            "Iter-48000 loss: 0.1685 validation: 0.970000\n",
            "Iter-48100 loss: 0.2775 validation: 0.970000\n",
            "Iter-48200 loss: 0.1042 validation: 0.970000\n",
            "Iter-48300 loss: 0.3827 validation: 0.970000\n",
            "Iter-48400 loss: 0.1118 validation: 0.970000\n",
            "Iter-48500 loss: 0.1845 validation: 0.970000\n",
            "Iter-48600 loss: 0.1960 validation: 0.970000\n",
            "Iter-48700 loss: 0.1724 validation: 0.970000\n",
            "Iter-48800 loss: 0.0899 validation: 0.970000\n",
            "Iter-48900 loss: 0.2387 validation: 0.970000\n",
            "Iter-49000 loss: 0.2752 validation: 0.970000\n",
            "Iter-49100 loss: 0.1273 validation: 0.970000\n",
            "Iter-49200 loss: 0.1640 validation: 0.970000\n",
            "Iter-49300 loss: 0.2454 validation: 0.970000\n",
            "Iter-49400 loss: 0.3081 validation: 0.970000\n",
            "Iter-49500 loss: 0.1274 validation: 0.970000\n",
            "Iter-49600 loss: 0.1409 validation: 0.970000\n",
            "Iter-49700 loss: 0.1689 validation: 0.970000\n",
            "Iter-49800 loss: 0.2449 validation: 0.970000\n",
            "Iter-49900 loss: 0.2187 validation: 0.970000\n",
            "Iter-50000 loss: 0.1362 validation: 0.970000\n",
            "\n",
            "Best max_norm:  4.0 best dropout rate for input:  1.0 best dropout rate for hidden layers:  1.0 and result accuracy is:  0.9554\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}